{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is a generic script for finetuning VisualBERT model for VQA tasks\n",
        "Simply format your data as per the requirements below and this should work."
      ],
      "metadata": {
        "id": "VBuK4zOLCrEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNEY3RdiynHE",
        "outputId": "b143187e-fe81-4df8-dc1b-dcc463534411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data format\n",
        "* the images (stored in a single folder)\n",
        "* the questions (stored in a JSON) -- with following\n",
        "* the annotations (stored in a JSON) a.k.a. the answers to the questions.\n",
        "\n",
        "Refer to https://github.com/multimodal/multimodal/blob/master/test/data/vqa2/val/v2_OpenEnded_mscoco_val2014_questions.json for a sample of how question json file should look like."
      ],
      "metadata": {
        "id": "boe1Duge6Jsa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CEYz9215-vH"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "home = '/content/drive/MyDrive/ReadyForFineTuning/MultiModalQA/'\n",
        "imroot = home+'merged_images'\n",
        "os.chdir(home)"
      ],
      "metadata": {
        "id": "P0a9maoNyrja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This piece of code converts data in VLQAv1 into desired format. If multiple images are there, it automatically merges them into one."
      ],
      "metadata": {
        "id": "rE7dk9JfLKvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = 'mmqa'"
      ],
      "metadata": {
        "id": "SEJyeCl-IjN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import ast\n",
        "\n",
        "dumpdict = { \"info\": {}, \"task_type\": \"Open-Ended\", \"data_type\": \"mmqa\", \"license\": {}, \"data_subtype\": prefix,\n",
        "    \"questions\": [] }\n",
        "\n",
        "with open(home+prefix+'.jsonl') as f:\n",
        "  data = f.readlines()\n",
        "  print(len(data))\n",
        "  for i in data:\n",
        "    ijson = json.loads(i)\n",
        "    imlist = ast.literal_eval(ijson['images'])\n",
        "    anslist = [str(i) for i in ast.literal_eval(ijson['answer_choices'])]\n",
        "    #if len(imlist) == 2:\n",
        "    #  mpath = mergeim(imlist[0], imlist[1])\n",
        "    # mpath\n",
        "    dumpdict['questions'].append( { \"image_id\": imroot+'/Merged_'+imlist[0].replace(\"./images/\",\"\",1).replace(\".png\",\"\",1)+\"#\"+imlist[1].replace(\"./images/\",\"\",1), \"question\": ijson['question']+\" \"+ijson['passage'], \"question_id\": ijson['qid'], \"answer_choices\": anslist, \"answer_id\": int(ijson['answer']) } )\n",
        "\n",
        "with open('/content/'+prefix+'_vbertft.jsonl','w+') as w:\n",
        "  w.write(json.dumps(dumpdict,indent=4))\n",
        "\n",
        "print(dumpdict)"
      ],
      "metadata": {
        "id": "bWWyIF0gEb3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read questions\n",
        "\n",
        "First, we read the questions."
      ],
      "metadata": {
        "id": "QgTMR9H5kkHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "f = open('/content/'+prefix+'_vbertft.jsonl','r')\n",
        "data_questions = json.load(f)\n",
        "questions = data_questions['questions']\n",
        "\n",
        "print(data_questions.keys())\n",
        "print(\"Number of questions:\", len(questions))"
      ],
      "metadata": {
        "id": "XKAWRzGk7mPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's quite a lot! Let's take a look at the first one:"
      ],
      "metadata": {
        "id": "Uigy071flj4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions[0]"
      ],
      "metadata": {
        "id": "6xKiXtLz75co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "# -- CHANGE THIS BASED ON WHERE IMAGES ARE STORED --\n",
        "#root = home+'images'\n",
        "\n",
        "file_names = [f for f in listdir(imroot) if isfile(join(imroot, f))]\n",
        "print(file_names)\n",
        "print(len(file_names))"
      ],
      "metadata": {
        "id": "LxBw6v-V9Zx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "path = questions[0]['image_id']\n",
        "image = Image.open(path)\n",
        "image"
      ],
      "metadata": {
        "id": "ZeW4M0yU-boe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, VisualBertForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "config = VisualBertForQuestionAnswering.from_pretrained(\"uclanlp/visualbert-vqa\")"
      ],
      "metadata": {
        "id": "qQK-4nIa_6SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "class VQADataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, questions, processor): #annotations\n",
        "        self.questions = questions\n",
        "        #self.annotations = annotations\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get image + text\n",
        "        #annotation = self.annotations[idx]\n",
        "        questions = self.questions[idx]\n",
        "        #image = Image.open(id_to_filename[annotation['image_id']])\n",
        "        image = Image.open(questions['image_id']).convert(\"RGB\")\n",
        "        text = questions['question']\n",
        "        #print(image)\n",
        "        #print(text)\n",
        "        encoding = self.processor(image, text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        # remove batch dimension\n",
        "        for k,v in encoding.items():\n",
        "          encoding[k] = v.squeeze()\n",
        "        # add labels\n",
        "        #labels = questions['answer_choices'][questions['answer_id']] #annotation['labels']\n",
        "        #scores = [1.0] #annotation['scores']\n",
        "        #print(len(questions['answer_choices']))\n",
        "        targets = torch.zeros(len(questions['answer_choices']))\n",
        "        targets[questions['answer_id']] = 1.0\n",
        "        #print(targets)\n",
        "        #for label, score in zip(labels, scores):\n",
        "        #      targets[label] = score\n",
        "        encoding[\"labels\"] = targets\n",
        "        #print(encoding)\n",
        "        #print(labels)\n",
        "        #print(scores)\n",
        "\n",
        "        return encoding"
      ],
      "metadata": {
        "id": "kJ3sm7qGAyab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import VisualBertForQuestionAnswering\n",
        "\n",
        "processor = VisualBertForQuestionAnswering.from_pretrained(\"uclanlp/visualbert-vqa\")"
      ],
      "metadata": {
        "id": "Dl2UsPrTHbtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = VQADataset(questions=questions,processor=processor) ##annotations=annotations[:10],"
      ],
      "metadata": {
        "id": "EagfM_HDRTLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1].keys()"
      ],
      "metadata": {
        "id": "bKFd8ozdIY_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor.decode(dataset[0]['input_ids'])"
      ],
      "metadata": {
        "id": "F0xtCsxr2B4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = torch.nonzero(dataset[0]['labels']).squeeze().tolist()"
      ],
      "metadata": {
        "id": "AGxYDVSve-O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import VisualBertForQuestionAnswering\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = VisualBertForQuestionAnswering.from_pretrained(\"uclanlp/visualbert-vqa\", num_labels=2)\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "sjOH0jYcfkAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "  input_ids = [item['input_ids'] for item in batch]\n",
        "  pixel_values = [item['pixel_values'] for item in batch]\n",
        "  #print(pixel_values)\n",
        "  attention_mask = [item['attention_mask'] for item in batch]\n",
        "  token_type_ids = [item['token_type_ids'] for item in batch]\n",
        "  labels = [item['labels'] for item in batch]\n",
        "\n",
        "  # create padded pixel values and corresponding pixel mask\n",
        "  #encoding = processor.feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n",
        "\n",
        "  # create new batch\n",
        "  batch = {}\n",
        "  batch['input_ids'] = torch.stack(input_ids)\n",
        "  batch['attention_mask'] = torch.stack(attention_mask)\n",
        "  batch['token_type_ids'] = torch.stack(token_type_ids)\n",
        "  batch['pixel_values'] = torch.stack(pixel_values) #encoding['pixel_values']\n",
        "  #batch['pixel_mask'] = encoding['pixel_mask']\n",
        "  batch['labels'] = torch.stack(labels)\n",
        "\n",
        "  return batch\n",
        "\n",
        "train_dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "9C4gCxrbgjqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's verify a batch:"
      ],
      "metadata": {
        "id": "bvOs133CxoP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "yKBOBxq1gqzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,v in batch.items():\n",
        "  print(k, v.shape)"
      ],
      "metadata": {
        "id": "lZkar-taiq1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a model\n",
        "\n",
        "Finally, let's train a model!"
      ],
      "metadata": {
        "id": "uN8Grgg0gK2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# -- CHANGE BASED ON HOW MANY EPOCHS YOU WANT TO RUN FOR\n",
        "e = 5\n",
        "\n",
        "model.train()\n",
        "for epoch in range(e):  # loop over the dataset multiple times\n",
        "   print(f\"Epoch: {epoch}\")\n",
        "   for batch in tqdm(train_dataloader):\n",
        "        # get the inputs;\n",
        "        batch = {k:v.to(device) for k,v in batch.items()}\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        print(\"Loss:\", loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "57PlqyxXf6L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "Let's verify whether the model has actually learned something:"
      ],
      "metadata": {
        "id": "aj5anZ-ZyMEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single Instance Inference"
      ],
      "metadata": {
        "id": "4SkLrjIRCJhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exid = 1\n",
        "example = dataset[exid]\n",
        "print(example.keys())"
      ],
      "metadata": {
        "id": "qG0UzoCfgpEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add batch dimension + move to GPU\n",
        "example = {k: v.unsqueeze(0).to(device) for k,v in example.items()}\n",
        "\n",
        "# forward pass\n",
        "outputs = model(**example)"
      ],
      "metadata": {
        "id": "eszu33n7yS4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = outputs.logits\n",
        "predicted_class = logits.argmax(-1).item()\n",
        "print(\"Predicted answer:\", predicted_class)\n",
        "print(\"Ground-truth answer:\", dataset[exid]['labels'].argmax(-1).item())"
      ],
      "metadata": {
        "id": "xZ4_ceydyePt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Inference"
      ],
      "metadata": {
        "id": "u9KPb-kxCNC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "tasktype = \"2way\"\n",
        "filprefix = prefix+\"_\"+tasktype\n",
        "\n",
        "count = 0\n",
        "st = 0\n",
        "end = 251\n",
        "with open(filprefix+'.csv', 'w+') as k:\n",
        "  spamwriter = csv.writer(k)\n",
        "  spamwriter.writerow([\"qid\",\"pred_ans\",\"gt_ans\",\"correctness\"])\n",
        "  for exid in range(st,end):\n",
        "    correctness = 0\n",
        "    example = dataset[exid]\n",
        "    example = {k: v.unsqueeze(0).to(device) for k,v in example.items()}\n",
        "    outputs = model(**example)\n",
        "    logits = outputs.logits\n",
        "    predicted_class = logits.argmax(-1).item()\n",
        "    gtclass = dataset[exid]['labels'].argmax(-1).item()\n",
        "    if gtclass==predicted_class:\n",
        "      count+=1\n",
        "      correctness = 1\n",
        "    #print([exid,predicted_class, gtclass,correctness])\n",
        "    spamwriter.writerow([exid,predicted_class, gtclass,correctness])"
      ],
      "metadata": {
        "id": "KOOVAOc5yvbT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}